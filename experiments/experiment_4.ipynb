{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: LLava Approach -Clip Encoder/Vicuna Decoder with LoRA\n",
    "\n",
    "This experiment tests the LLava style projections using the HuggingFace Clip encoder and the Vicuna 7b decoder. The experiment has two stages as outlined in the LLAva paper.\n",
    "\n",
    "- **Stage One**: pretrains with the encoder and decoder frozen. The only trainable parameters at this stage are in the MLP used for image projections.\n",
    "\n",
    "- **Stage Two**: performs fine-tuning using low rank adaption for the Vicuna model with the mlp for image projection unfrozen. The pretrained MLP weights from stage one are loaded as the starting point for the MLP in stage two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer\n",
    "import datetime\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "\"\"\"\n",
    "Add support for either running in collab by uploading this notebook and\n",
    "mounting the directory or locally from the room or experiments folder\"\n",
    "\"\"\"\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # NOTE: change the drive path if running with a mounted google drive in collab\n",
    "    project_root = \"/content/drive/Othercomputers/My MacBook Pro/image-captioning\"\n",
    "else:\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    if cwd.endswith(\"experiments\"):\n",
    "        project_root = os.path.abspath(os.path.join(cwd, '..'))\n",
    "    else:\n",
    "        project_root = cwd\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install evaluate > /dev/null 2>&1\n",
    "    !pip install pycocoevalcap > /dev/null 2>&1\n",
    "\n",
    "\n",
    "from vision_language_model import VisionLanguageModel\n",
    "import train as train\n",
    "import data_processing as dp\n",
    "import download_data as get_data\n",
    "import evaluation as eval\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "experiment = \"experiment_4\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# download data, keep it outside of the mounted directory if running in collab to avoid data transfer overhead\n",
    "if IN_COLAB:\n",
    "    data_dir = \"/content/flickr30k_data\"\n",
    "else:\n",
    "    data_dir = os.path.join(project_root, \"flickr30k_data\")\n",
    "\n",
    "# only download data if it does not already exist\n",
    "if not os.path.exists(data_dir) or not os.listdir(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    get_data.download_and_partition(data_dir)\n",
    "else:\n",
    "    print(f\"Data already exists in {data_dir}, skipping download.\")\n",
    "\n",
    "# setup saving directories\n",
    "model_weights_dir = os.path.join(project_root, \"model_weights\")\n",
    "evaluations_dir = os.path.join(project_root, \"evaluations\")\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "os.makedirs(evaluations_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# Note: the train loader loads duplicate images with a 1:1 mapping of image to captions\n",
    "# and the val/test loaders load images with a 1:N mapping of image to captions for evaluation\n",
    "# these loaders will load batches of images rather than all images at once to avoid memory issues\n",
    "train_loader = dp.batch_stream(\"captions.txt\", os.path.join(data_dir, \"train\"), batch_size=4, eval_mode=False)\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# visualize some training images\n",
    "batch_1 = next(train_loader)\n",
    "batch_2 = next(train_loader)\n",
    "dp.visualize_random_captions([batch_1, batch_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Pretraining MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model config for stage one\n",
    "model = VisionLanguageModel(\n",
    "    image_encoder_type=\"clip\",\n",
    "    llava_projections=True,\n",
    "    cross_attention=False,\n",
    "    debug=False,\n",
    "    decoder_type=\"vicuna\",\n",
    "    d_model=4096,\n",
    "    tokenizer=tokenizer\n",
    "    ).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# stage one: pretraining\n",
    "\n",
    "max_batches=10000\n",
    "num_epochs=1\n",
    "\n",
    "train.train(\n",
    "    model=model,\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    loss_function=loss_fn,\n",
    "    device=device,\n",
    "    batch_size=2,\n",
    "    num_epochs=num_epochs,\n",
    "    training_type=\"mlp-pretrain\",\n",
    "    log_interval=1000,\n",
    "    max_batches=max_batches,\n",
    "    lr_scheduler=True,\n",
    "    random_seed=1,\n",
    "    learning_rate=2e-3,\n",
    "    mlp_weights_path=os.path.join(model_weights_dir, f\"{experiment}_pretrain_weights_{timestamp}.pt\"),\n",
    "    loss_plot_path=os.path.join(evaluations_dir, f\"{experiment}_pretrain_loss_{timestamp}.jpg\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_epochs=2\n",
    "max_batches=2000\n",
    "\n",
    "train.train(\n",
    "    model=model,\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    loss_function=loss_fn,\n",
    "    device=device,\n",
    "    batch_size=2,\n",
    "    num_epochs=num_epochs,\n",
    "    training_type=\"lora\",\n",
    "    max_batches=max_batches,\n",
    "    log_interval=500,\n",
    "    learning_rate=2e-4,\n",
    "    random_seed=16,\n",
    "    eval_every=2,\n",
    "    model_weights_path=os.path.join(model_weights_dir, f\"{experiment}_finetune_weights_{timestamp}.pt\"),\n",
    "    loss_plot_path=os.path.join(evaluations_dir, f\"{experiment}_finetune_loss_{timestamp}.jpg\"),\n",
    "    all_epochs_loss_plot_path=os.path.join(evaluations_dir, f\"{experiment}_all_epochs_loss_{timestamp}.jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = dp.batch_stream(\"captions.txt\", os.path.join(data_dir, \"test\"), batch_size=2, eval_mode=True, seed=32)\n",
    "\n",
    "bleu, cider = eval.evaluate_bleu_cider(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    display_captions=True,\n",
    "    save_captions_path=os.path.join(evaluations_dir, f\"{experiment}_captions_{timestamp}.jpg\"),\n",
    "    max_new_tokens=20,\n",
    "    max_batches=500\n",
    ")\n",
    "\n",
    "print(f\"BLEU: {bleu:.4f}, CIDEr: {cider:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# save experiment results\n",
    "results_path = os.path.join(evaluations_dir, \"results.csv\")\n",
    "\n",
    "eval.save_experiment_results(experiment, cider, bleu, results_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
